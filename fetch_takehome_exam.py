# -*- coding: utf-8 -*-
"""Fetch_TakeHome_exam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jC8dCtF6KMLHComTyukmc6bVnLgczgth

ML Apprenticeship Take-Home
Sentence Transformers and Multi-Task Learning

#### Task 1: Sentence Transformer Implementation
Implement a sentence transformer model using any deep learning framework of your choice. This model should be able to encode input sentences into fixed-length embeddings. Test your implementation with a few sample sentences and showcase the obtained embeddings.
Describe any choices you had to make regarding the model architecture outside of the transformer backbone.
"""

#Import all the lobraries neeeded

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer
from transformers import DistilBertTokenizer, DistilBertModel

import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import os
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from transformers import BertTokenizer

'''
DistilBERT is chosen as the transformer backbone due to its computational efficiency and competitive performance compared to the larger BERT model.
This choice is particularly helpful for applications with limited computational resources where real-time inference is required.

Transformers: The Hugging Face transformers library is used here , which gives us easy access to pre-trained transformer models like BERT,
including tokenizers and model classes.

embedding_dim=768: The dimension of the output sentence embeddings. By default, it is set to 768, matching BERT's hidden size.
Here the output of our model has the embeddings for each token in the sequence have a fixed length.

The forward method processes the input through DistilBERT and then through the projection layer to get the sentence embeddings.
The sentence embeddings are then returned.

A linear layer is added after DistilBERT to allow for dimension reduction or transformation of the embeddings.
Here our projection layer keeps the dimension same as the output dimension of DistilBERT (768).

[CLS] pool startegy is a common used pooling texhnique: token's embedding from the last hidden state to represent the sentence embedding is used here
We adopted this as it aligns with the pre-training objective of these models, where token is used for sequence-level tasks such as classification

'''

class SentenceTransformer(nn.Module):
    def __init__(self, model_name="distilbert-base-uncased", embedding_dim=768):
        #Initializes the class
        super(SentenceTransformer, self).__init__()

        #Loads a pre-trained DistilBERT model
        self.bert = DistilBertModel.from_pretrained(model_name)

        #Defines a linear layer for projecting the encoded representation from the BERT model to the desired embedding dimension

        self.projection = nn.Linear(self.bert.config.hidden_size, embedding_dim)

    def forward(self, input_ids, attention_mask):
      #This method defines the forward pass of the model, taking input IDs and attention mask as arguments
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        #outputs contains the last hidden state of the BERT model
        last_hidden_state = outputs.last_hidden_state

        sentence_embeddings = self.projection(last_hidden_state[:, 0, :]) #pooling startegy
        return sentence_embeddings

# Example usage for better undertsanding on the sentnces are converted into word embeddings
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = SentenceTransformer()

sentences = ["Hi", "How are you","This is a sample", "This is a sentence.","This is to check"]
encoded_input = tokenizer(sentences, padding= True, truncation=True, return_tensors='pt')
embeddings = model(encoded_input['input_ids'], encoded_input['attention_mask'])

print(embeddings.shape)
print(embeddings)
#The output of the model is a tuple containing the last hidden state, which represents the contextualized token embeddings.

# Task 2: Multi-Task Learning Expansion
'''
Multi-task model can  handle multiple tasks without the need to retrain the model for each task making it computationally effcient.
The primary modification we followed here is  adding separate task-specific heads to handle different tasks,
such as Task_a: sentence classification and  Task_b sentiment analysis
Two separate linear layers are added after the projection layer. These heads are responsible for producing task-specific outputs.

The forward method  accepts an  argument, task, which specifies which task's head should be used to produce the final output.
Depending on the value of task, the model routes the sentence embeddings through the appropriate task head.

Here we have used the sam eabove cell task1 implementation but slighlt modified the approach for multi-task learning'''
from transformers import DistilBertModel, DistilBertTokenizer

class MultiTaskSentenceTransformer(nn.Module):
    def __init__(self, model_name="distilbert-base-uncased", embedding_dim=768, num_classes_task_a=5, num_classes_task_b=3):
        super(MultiTaskSentenceTransformer, self).__init__()
        self.bert = DistilBertModel.from_pretrained(model_name)
        self.projection = nn.Linear(self.bert.config.hidden_size, embedding_dim) # A linear layer to project the BERT output to a fixed embedding dimension
        self.task_a_head = nn.Linear(embedding_dim, num_classes_task_a)  # Sentence Classification
        self.task_b_head = nn.Linear(embedding_dim, num_classes_task_b)  # Sentiment Analysis

    def forward(self, input_ids, attention_mask, task):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask) #Get the outputs from the BERT model.
        last_hidden_state = outputs.last_hidden_state #Extract the last hidden state from the outputs.
        sentence_embeddings = self.projection(last_hidden_state[:, 0, :])#Use the first token embedding from the last hidden state and project it to the desired embedding dimension

        if task == "task_a":
            logits = self.task_a_head(sentence_embeddings)
        elif task == "task_b":
            logits = self.task_b_head(sentence_embeddings)
        else:
            raise ValueError("Invalid task specified.")

        return logits


# Example usage
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = MultiTaskSentenceTransformer() #used default classes size for convenience and easeness uses 5 classes for task1, and 3 classes for task2

# Sample sentences
sentences = ["Hi", "How are you","This is a sample", "This is a sentence.","This is to check"]

# Tokenize input
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# Forward pass for Task A (Sentence Classification)
logits_task_a = model(encoded_input['input_ids'], encoded_input['attention_mask'], task="task_a")
print("Logits for Task A:", logits_task_a)

# Forward pass for Task B (Sentiment Analysis)
logits_task_b = model(encoded_input['input_ids'], encoded_input['attention_mask'], task="task_b")
print("Logits for Task B:", logits_task_b)

"""##Task 3


#### If the entire network should be frozen.

1. Freezing the entire network means no parameters will be updated during training, effectively turning the model into a fixed feature extractor. Freezing the entire network is generally not recommended for multi-task learning scenarios. It should only be considered if the pre-trained representations are already highly optimized for the target tasks.


---

#### If only the transformer backbone should be frozen.

2. Freezing the transformer backbone is a common practice in transfer learning for NLP tasks. It allows the model to leverage the general language representations learned during pre-training while fine-tuning the task-specific components.


---

#### If only one of the task-specific heads (either for Task A or Task B) should be frozen.
3. Freezing one of the task-specific heads can be useful in scenarios where you want to transfer knowledge from a well-performing model for one task to improve performance on another task. For example, if you have a highly accurate model for Task A, you could freeze the Task A head and fine-tune the rest of the model on Task B, leveraging the knowledge from Task A.

###Transfer Learning Scenario:

---
When doing transfer learning for NLP tasks, it's important to pick a pre-trained model like **BERT, RoBERTa, XLNet, or GPT**, which has been trained on relevant data.

An effective approach is to **freeze the transformer backbone and fine-tune the task-specific heads and the projection layer**.

Unfreezing a few transformer backbone layers can help the model adapt its language representations to specific tasks or domains, especially if they're significantly different from the pre-training data.


---

My approach:

1. Freezing the transformer backbone reduces the number of trainable parameters, making the fine-tuning process more computationally efficient and less prone to overfitting, especially when dealing with limited task-specific data.

2. Fine-tuning the task-specific heads and the projection layer allows the model to adapt to the specific tasks and learn task-relevant representations.

3. Unfreezing a few layers of the transformer backbone can help the model adapt its language representations to the specific tasks or domains, potentially improving performance if the target tasks are significantly different from the pre-training data.
---
"""

#Task 4: Layer-wise Learning Rate Implementation (BONUS)

'''
Lower learning rates are typically set for the lower layers of the network because they often capture more general features and require more stable updates.
Slightly higher learning rates can be set for the middle layers to ensure faster adaptation to task-specific features.
Higher learning rates are set for the task-specific heads (task_a_head and task_b_head) to facilitate faster convergence on the specific task

Advantages  of using layer wise learning rates:
Improved stability and faster convergence.
Different layers of a neural network may learn at different rates or require different magnitudes of updates to their parameters.
Layer-wise learning rates allow us to adjust the learning rates for each layer individually, providing finer control over the learning process.
This can help accelerate convergence and improve overall training performance.
Applying different learning rates to different layers can act as a form of regularization to prevent overfitting.


'''

model = MultiTaskSentenceTransformer()

# Define different learning rates for different layers
learning_rates = [
    {"params": model.bert.parameters(), "lr": 1e-5},        # Lower learning rate for BERT layers
    {"params": model.projection.parameters(), "lr": 1e-4},  # Slightly higher learning rate for projection layer
    {"params": model.task_a_head.parameters(), "lr": 1e-3},  # Higher learning rate for task A head
    {"params": model.task_b_head.parameters(), "lr": 1e-3}   # Higher learning rate for task B head
]

# optimizer with different learning rates for different layers
optimizer = optim.Adam(learning_rates)



"""#### My understanding and Summarization Task 3: Training Considerations
When freezing the entire network, only the task-specific heads are trainable, using pre-trained embeddings for quick and computationally efficient . Gradients are computed only for the task-specific heads, no backpropagation through the transformer layers.

Freezing only the transformer backbone preserves general linguistic features while allowing task-specific heads to specialize, balancing computational efficiency and adaptability. Gradients flow through the transformer to compute the task-specific head updates, but transformer weights remain unchanged

Freezing one task-specific head enables incremental learning, maintaining performance on one task while adapting to another. Gradients affect the transformer and one task-specific head, leaving the other head's weights unchanged

For transfer learning, using a pre-trained model freeze the initial layers (first 6) to retain general features and** unfreeze higher layers** and task-specific heads to adapt to new tasks, ensuring robust performance with minimal training time.


---



####Task 4: Layer-wise Learning Rate Implementation (BONUS)

Implementing layer-wise learning rates involves setting lower rates (1e-5) for initial transformer layers to maintain pre-trained stability, higher rates ( 5e-5) for higher layers to adapt task-specific features, and the highest rates ( 1e-4) for task-specific heads for rapid learning.

This approach optimizes training efficiency and performance, preserving foundational knowledge while enabling quick adaptation to specific tasks, which is especially beneficial in multi-task learning to balance shared and task-specific learning.

"""

